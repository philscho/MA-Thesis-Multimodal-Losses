{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rootutils\n",
    "rootutils.setup_root('.', indicator=\".project-root\", pythonpath=True)\n",
    "from src.model.model_module import LitMML, MLMWrapper\n",
    "from src.model.utils import get_model_and_processor\n",
    "from omegaconf import OmegaConf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ckpt_path = \"/home/data/arena-multimodal/mlm_ckpts/mkqp8hhx/ckpt-epoch=74-loss-val=0.885.ckpt\"\n",
    "image_ckpt_path = \"/home/data/arena-multimodal/mlm_ckpts/txva2y48/ckpt-epoch=79-loss-val=1.435.ckpt\"\n",
    "\n",
    "text_ckpt = torch.load(text_ckpt_path, map_location='cpu')\n",
    "image_ckpt = torch.load(image_ckpt_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.2.3, which is newer than your current Lightning version: v2.2.0.post0\n"
     ]
    }
   ],
   "source": [
    "model, processor = get_model_and_processor(config=OmegaConf.load('configs/model/dual_encoder.yaml'))\n",
    "model = MLMWrapper(model)\n",
    "lit_model = LitMML.load_from_checkpoint(image_ckpt_path, model=model, processor=processor)\n",
    "model, processor = lit_model.model, lit_model.processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.basemodel.text_model.embeddings.word_embeddings.weight', 'model.basemodel.text_model.embeddings.position_embeddings.weight', 'model.basemodel.text_model.embeddings.token_type_embeddings.weight', 'model.basemodel.text_model.embeddings.LayerNorm.weight', 'model.basemodel.text_model.embeddings.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.0.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.0.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.0.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.0.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.0.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.0.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.0.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.0.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.0.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.0.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.0.output.dense.weight', 'model.basemodel.text_model.encoder.layer.0.output.dense.bias', 'model.basemodel.text_model.encoder.layer.0.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.0.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.1.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.1.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.1.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.1.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.1.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.1.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.1.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.1.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.1.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.1.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.1.output.dense.weight', 'model.basemodel.text_model.encoder.layer.1.output.dense.bias', 'model.basemodel.text_model.encoder.layer.1.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.1.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.2.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.2.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.2.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.2.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.2.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.2.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.2.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.2.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.2.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.2.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.2.output.dense.weight', 'model.basemodel.text_model.encoder.layer.2.output.dense.bias', 'model.basemodel.text_model.encoder.layer.2.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.2.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.3.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.3.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.3.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.3.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.3.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.3.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.3.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.3.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.3.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.3.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.3.output.dense.weight', 'model.basemodel.text_model.encoder.layer.3.output.dense.bias', 'model.basemodel.text_model.encoder.layer.3.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.3.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.4.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.4.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.4.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.4.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.4.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.4.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.4.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.4.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.4.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.4.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.4.output.dense.weight', 'model.basemodel.text_model.encoder.layer.4.output.dense.bias', 'model.basemodel.text_model.encoder.layer.4.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.4.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.5.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.5.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.5.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.5.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.5.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.5.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.5.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.5.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.5.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.5.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.5.output.dense.weight', 'model.basemodel.text_model.encoder.layer.5.output.dense.bias', 'model.basemodel.text_model.encoder.layer.5.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.5.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.6.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.6.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.6.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.6.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.6.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.6.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.6.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.6.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.6.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.6.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.6.output.dense.weight', 'model.basemodel.text_model.encoder.layer.6.output.dense.bias', 'model.basemodel.text_model.encoder.layer.6.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.6.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.7.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.7.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.7.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.7.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.7.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.7.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.7.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.7.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.7.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.7.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.7.output.dense.weight', 'model.basemodel.text_model.encoder.layer.7.output.dense.bias', 'model.basemodel.text_model.encoder.layer.7.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.7.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.8.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.8.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.8.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.8.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.8.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.8.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.8.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.8.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.8.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.8.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.8.output.dense.weight', 'model.basemodel.text_model.encoder.layer.8.output.dense.bias', 'model.basemodel.text_model.encoder.layer.8.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.8.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.9.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.9.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.9.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.9.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.9.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.9.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.9.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.9.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.9.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.9.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.9.output.dense.weight', 'model.basemodel.text_model.encoder.layer.9.output.dense.bias', 'model.basemodel.text_model.encoder.layer.9.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.9.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.10.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.10.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.10.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.10.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.10.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.10.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.10.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.10.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.10.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.10.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.10.output.dense.weight', 'model.basemodel.text_model.encoder.layer.10.output.dense.bias', 'model.basemodel.text_model.encoder.layer.10.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.10.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.11.attention.self.query.weight', 'model.basemodel.text_model.encoder.layer.11.attention.self.query.bias', 'model.basemodel.text_model.encoder.layer.11.attention.self.key.weight', 'model.basemodel.text_model.encoder.layer.11.attention.self.key.bias', 'model.basemodel.text_model.encoder.layer.11.attention.self.value.weight', 'model.basemodel.text_model.encoder.layer.11.attention.self.value.bias', 'model.basemodel.text_model.encoder.layer.11.attention.output.dense.weight', 'model.basemodel.text_model.encoder.layer.11.attention.output.dense.bias', 'model.basemodel.text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'model.basemodel.text_model.encoder.layer.11.intermediate.dense.weight', 'model.basemodel.text_model.encoder.layer.11.intermediate.dense.bias', 'model.basemodel.text_model.encoder.layer.11.output.dense.weight', 'model.basemodel.text_model.encoder.layer.11.output.dense.bias', 'model.basemodel.text_model.encoder.layer.11.output.LayerNorm.weight', 'model.basemodel.text_model.encoder.layer.11.output.LayerNorm.bias', 'model.basemodel.text_model.pooler.dense.weight', 'model.basemodel.text_model.pooler.dense.bias', 'model.basemodel.text_projection.weight'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = text_ckpt['state_dict']\n",
    "for key in list(state_dict.keys()):\n",
    "    if not key.startswith(\"model.basemodel.text\"):\n",
    "        state_dict.pop(key)\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.basemodel.text_projection.load_state_dict({\"weight\": state_dict.pop(\"model.basemodel.text_projection.weight\")})\n",
    "model.basemodel.text_model.load_state_dict({k.replace(\"model.basemodel.text_model.\", \"\"): v for k, v in state_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zeroshot_CIFAR10': {'1_templates': {'ConfusionMatrix': array([[  28,  140,    0, 4432,    1,  137,  252,    2,    6,    2],\n",
       "          [ 233,  178,   18, 3754,  102,  200,  284,   79,   98,   54],\n",
       "          [  44,  125,    7, 4587,    3,   27,  207,    0,    0,    0],\n",
       "          [  33,  166,   11, 4437,    4,  199,  139,    5,    1,    5],\n",
       "          [  56,   59,    7, 4766,    1,   20,   89,    0,    1,    1],\n",
       "          [ 156,  186,    3, 4362,    9,  174,   76,   18,   11,    5],\n",
       "          [  14,   13,    7, 4900,    1,    6,   58,    1,    0,    0],\n",
       "          [   6,   55,    0, 4336,    0,  251,  344,    2,    0,    6],\n",
       "          [  16,  147,    0, 4454,    0,  163,  169,    9,   35,    7],\n",
       "          [  66,  131,    6, 3777,  105,  382,  157,  136,  214,   26]]),\n",
       "   'Top1Accuracy': 0.09892000257968903,\n",
       "   'Top3Accuracy': 0.3257400095462799,\n",
       "   'Top5Accuracy': 0.532039999961853}},\n",
       " '__config__': {'model': {'model': {'image_encoder_name': 'google/vit-base-patch16-224', 'text_encoder_name': 'google-bert/bert-base-uncased', 'tokenizer': {'use_fast': False}}}, 'data': {'datasets': ['CIFAR10'], 'dataloader': {'coco_val': {'batch_size': 128, 'shuffle': False}, 'test': {'batch_size': 128, 'shuffle': False, 'num_workers': 8, 'pin_memory': True}}, 'root': '${paths.data_dir}', 'Places365': {'root': '/home/data/places365/'}, 'use_subset_probe': {'value': False, 'subset_fraction': 0.1}, 'dataset': {'val': ['coco_val_dummy'], 'transforms': {'enabled': True, 'RandAugment': {'num_ops': 3, 'magnitude': 8}}, 'max_seq_length': 72, 'coco': {'root': '/home/data/mscoco_captions/', 'split_train': '/home/phisch/multimodal/my_datasets/coco_karpathy_train.json', 'split_val': '${paths.root_dir}src/data/datasets/coco_karpathy_val.json', 'split_test': '/home/phisch/multimodal/my_datasets/coco_karpathy_test.json'}, 'vg': {'data_dir': '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/shared/datasets/VG_Bhavin/VG'}, 'cc3m': {'data_dir': '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/shared/datasets/CC3m/h5'}, 'cifar10': {'root': '/home/phisch/data/cifar-10-batches-py', 'download': False}, 'caltech101': {'root': '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/shared/datasets/cifar10', 'download': False}, 'use_subset': {'value': True, 'subset_fraction': 0.2}, 'use_subset_probe': {'value': False, 'subset_fraction': 0.1}, 'label_as_caption': False, 'caption_template': '{}'}}, 'lightning': {'trainer': {'strategy': 'auto', 'fast_dev_run': False, 'log_every_n_steps': 1, 'max_epochs': 1, 'devices': '${device}', 'num_nodes': 1, 'accelerator': 'gpu', 'deterministic': 'warn', 'precision': '16-mixed', 'gradient_clip_algorithm': 'norm', 'gradient_clip_val': 1.0}, 'seed': 69}, 'callbacks': {'zeroshot': {'templates': ['a photo of a {}.'], 'use_itm_head': False, 'eval_all_text_layers': False, 'callback': {'batch_size': '${data.dataloader.test.batch_size}', 'device': 'cuda', 'top_k': [1, 3, 5], 'top_k_preds': 20, 'average': 'micro', 'confusion_matrix': True, 'multi_label': False, 'verbose': True, 'templates': '${callbacks.zeroshot.templates}'}}, 'linear_probe': None}, 'loss': {'losses': ['contrastive', 'image_text_matching', 'SimCLR'], 'contrastive': None, 'image_text_matching': {'arg1': ''}}, 'optimizer': {'name': 'AdamW', 'lr': 2e-05, 'kwargs': {'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'scheduler': {'enabled': True, 'name': 'CosineWarmup', 'monitor': 'loss-val', 'interval': 'step', 'kwargs': {'initial_lr': 1e-08, 'num_warmup_steps': 'epoch', 'num_training_steps': 'all'}}, 'paths': {'root_dir': '${oc.env:PROJECT_ROOT}', 'data_dir': '/home/data/', 'log_dir': '${paths.root_dir}/logs/', 'work_dir': '${hydra:runtime.cwd}', 'ckpts_dir': '/home/data/bhavin/higher_augmentations_ckpts/'}, 'logger': {'wandb': {'entity': 'arena-multimodal-lossfns', 'project': 'multimodal', 'name': '', 'tags': ['model evaluation', 'zeroshot'], 'offline': False, 'group': '', 'dir': '${paths.root_dir}/wandb/'}}, 'checkpoints': {'3burj4np': {'image_encoder': 'google/vit-base-patch16-224', 'model': 'CLIP', 'subset_fraction': '1-aug', 'path': '/home/data/bhavin/higher_augmentations_ckpts/3burj4np/last.ckpt'}}, 'device': [3], 'result_subdir': 'different_modality_weights', 'result_file_suffix': 'zeroshot-results', 'different_modality_weights': {'ckpt_path': '/home/data/arena-multimodal/mlm_ckpts/mkqp8hhx/ckpt-epoch=74-loss-val=0.885.ckpt'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/home/phisch/multimodal/test_results/different_modality_weights/3burj4np-zeroshot-results.p\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
