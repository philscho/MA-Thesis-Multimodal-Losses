{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_cka import CKA\n",
    "import rootutils\n",
    "from omegaconf import OmegaConf\n",
    "from torchvision.transforms import RandAugment\n",
    "from hydra import compose, initialize\n",
    "\n",
    "ROOT = rootutils.setup_root(\"/home/phisch/multimodal\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.model.utils import get_model_and_processor\n",
    "from src.model.model_module import LitMML\n",
    "from src.data.data_module import MyDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2653382/2639836681.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"configs\"):\n"
     ]
    }
   ],
   "source": [
    "with initialize(config_path=\"configs\"):\n",
    "    cfg = compose(config_name=\"test\", overrides=[\"checkpoints=full_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = cfg.model\n",
    "ckpt_path_clip = \"/home/data/bhavin/ckpts/yh1adr3g/last.ckpt\"\n",
    "ckpt_path_clip_simclr_itm = \"/home/data/bhavin/ckpts/9nvg456i/last.ckpt\"\n",
    "ckpt_path_clip_itm = \"/home/data/bhavin/ckpts/93t3xgrr/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.2.3, which is newer than your current Lightning version: v2.2.0.post0\n"
     ]
    }
   ],
   "source": [
    "model, processor = get_model_and_processor(model_config)\n",
    "\n",
    "lit_model = LitMML.load_from_checkpoint(ckpt_path_clip, \n",
    "                                        model=model, \n",
    "                                        processor=processor,\n",
    "                                        )\n",
    "model_clip = lit_model.model\n",
    "\n",
    "lit_model = LitMML.load_from_checkpoint(ckpt_path_clip_itm, \n",
    "                                        model=model, \n",
    "                                        processor=processor,\n",
    "                                        )\n",
    "model_clip_itm = lit_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 vision_model\n",
      "2 vision_model.embeddings\n",
      "3 vision_model.embeddings.patch_embeddings\n",
      "4 vision_model.embeddings.patch_embeddings.projection\n",
      "5 vision_model.embeddings.dropout\n",
      "6 vision_model.encoder\n",
      "7 vision_model.encoder.layer\n",
      "8 vision_model.encoder.layer.0\n",
      "9 vision_model.encoder.layer.0.attention\n",
      "10 vision_model.encoder.layer.0.attention.attention\n",
      "11 vision_model.encoder.layer.0.attention.attention.query\n",
      "12 vision_model.encoder.layer.0.attention.attention.key\n",
      "13 vision_model.encoder.layer.0.attention.attention.value\n",
      "14 vision_model.encoder.layer.0.attention.attention.dropout\n",
      "15 vision_model.encoder.layer.0.attention.output\n",
      "16 vision_model.encoder.layer.0.attention.output.dense\n",
      "17 vision_model.encoder.layer.0.attention.output.dropout\n",
      "18 vision_model.encoder.layer.0.intermediate\n",
      "19 vision_model.encoder.layer.0.intermediate.dense\n",
      "20 vision_model.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "21 vision_model.encoder.layer.0.output\n",
      "22 vision_model.encoder.layer.0.output.dense\n",
      "23 vision_model.encoder.layer.0.output.dropout\n",
      "24 vision_model.encoder.layer.0.layernorm_before\n",
      "25 vision_model.encoder.layer.0.layernorm_after\n",
      "26 vision_model.encoder.layer.1\n",
      "27 vision_model.encoder.layer.1.attention\n",
      "28 vision_model.encoder.layer.1.attention.attention\n",
      "29 vision_model.encoder.layer.1.attention.attention.query\n",
      "30 vision_model.encoder.layer.1.attention.attention.key\n",
      "31 vision_model.encoder.layer.1.attention.attention.value\n",
      "32 vision_model.encoder.layer.1.attention.attention.dropout\n",
      "33 vision_model.encoder.layer.1.attention.output\n",
      "34 vision_model.encoder.layer.1.attention.output.dense\n",
      "35 vision_model.encoder.layer.1.attention.output.dropout\n",
      "36 vision_model.encoder.layer.1.intermediate\n",
      "37 vision_model.encoder.layer.1.intermediate.dense\n",
      "38 vision_model.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "39 vision_model.encoder.layer.1.output\n",
      "40 vision_model.encoder.layer.1.output.dense\n",
      "41 vision_model.encoder.layer.1.output.dropout\n",
      "42 vision_model.encoder.layer.1.layernorm_before\n",
      "43 vision_model.encoder.layer.1.layernorm_after\n",
      "44 vision_model.encoder.layer.2\n",
      "45 vision_model.encoder.layer.2.attention\n",
      "46 vision_model.encoder.layer.2.attention.attention\n",
      "47 vision_model.encoder.layer.2.attention.attention.query\n",
      "48 vision_model.encoder.layer.2.attention.attention.key\n",
      "49 vision_model.encoder.layer.2.attention.attention.value\n",
      "50 vision_model.encoder.layer.2.attention.attention.dropout\n",
      "51 vision_model.encoder.layer.2.attention.output\n",
      "52 vision_model.encoder.layer.2.attention.output.dense\n",
      "53 vision_model.encoder.layer.2.attention.output.dropout\n",
      "54 vision_model.encoder.layer.2.intermediate\n",
      "55 vision_model.encoder.layer.2.intermediate.dense\n",
      "56 vision_model.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "57 vision_model.encoder.layer.2.output\n",
      "58 vision_model.encoder.layer.2.output.dense\n",
      "59 vision_model.encoder.layer.2.output.dropout\n",
      "60 vision_model.encoder.layer.2.layernorm_before\n",
      "61 vision_model.encoder.layer.2.layernorm_after\n",
      "62 vision_model.encoder.layer.3\n",
      "63 vision_model.encoder.layer.3.attention\n",
      "64 vision_model.encoder.layer.3.attention.attention\n",
      "65 vision_model.encoder.layer.3.attention.attention.query\n",
      "66 vision_model.encoder.layer.3.attention.attention.key\n",
      "67 vision_model.encoder.layer.3.attention.attention.value\n",
      "68 vision_model.encoder.layer.3.attention.attention.dropout\n",
      "69 vision_model.encoder.layer.3.attention.output\n",
      "70 vision_model.encoder.layer.3.attention.output.dense\n",
      "71 vision_model.encoder.layer.3.attention.output.dropout\n",
      "72 vision_model.encoder.layer.3.intermediate\n",
      "73 vision_model.encoder.layer.3.intermediate.dense\n",
      "74 vision_model.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "75 vision_model.encoder.layer.3.output\n",
      "76 vision_model.encoder.layer.3.output.dense\n",
      "77 vision_model.encoder.layer.3.output.dropout\n",
      "78 vision_model.encoder.layer.3.layernorm_before\n",
      "79 vision_model.encoder.layer.3.layernorm_after\n",
      "80 vision_model.encoder.layer.4\n",
      "81 vision_model.encoder.layer.4.attention\n",
      "82 vision_model.encoder.layer.4.attention.attention\n",
      "83 vision_model.encoder.layer.4.attention.attention.query\n",
      "84 vision_model.encoder.layer.4.attention.attention.key\n",
      "85 vision_model.encoder.layer.4.attention.attention.value\n",
      "86 vision_model.encoder.layer.4.attention.attention.dropout\n",
      "87 vision_model.encoder.layer.4.attention.output\n",
      "88 vision_model.encoder.layer.4.attention.output.dense\n",
      "89 vision_model.encoder.layer.4.attention.output.dropout\n",
      "90 vision_model.encoder.layer.4.intermediate\n",
      "91 vision_model.encoder.layer.4.intermediate.dense\n",
      "92 vision_model.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "93 vision_model.encoder.layer.4.output\n",
      "94 vision_model.encoder.layer.4.output.dense\n",
      "95 vision_model.encoder.layer.4.output.dropout\n",
      "96 vision_model.encoder.layer.4.layernorm_before\n",
      "97 vision_model.encoder.layer.4.layernorm_after\n",
      "98 vision_model.encoder.layer.5\n",
      "99 vision_model.encoder.layer.5.attention\n",
      "100 vision_model.encoder.layer.5.attention.attention\n",
      "101 vision_model.encoder.layer.5.attention.attention.query\n",
      "102 vision_model.encoder.layer.5.attention.attention.key\n",
      "103 vision_model.encoder.layer.5.attention.attention.value\n",
      "104 vision_model.encoder.layer.5.attention.attention.dropout\n",
      "105 vision_model.encoder.layer.5.attention.output\n",
      "106 vision_model.encoder.layer.5.attention.output.dense\n",
      "107 vision_model.encoder.layer.5.attention.output.dropout\n",
      "108 vision_model.encoder.layer.5.intermediate\n",
      "109 vision_model.encoder.layer.5.intermediate.dense\n",
      "110 vision_model.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "111 vision_model.encoder.layer.5.output\n",
      "112 vision_model.encoder.layer.5.output.dense\n",
      "113 vision_model.encoder.layer.5.output.dropout\n",
      "114 vision_model.encoder.layer.5.layernorm_before\n",
      "115 vision_model.encoder.layer.5.layernorm_after\n",
      "116 vision_model.encoder.layer.6\n",
      "117 vision_model.encoder.layer.6.attention\n",
      "118 vision_model.encoder.layer.6.attention.attention\n",
      "119 vision_model.encoder.layer.6.attention.attention.query\n",
      "120 vision_model.encoder.layer.6.attention.attention.key\n",
      "121 vision_model.encoder.layer.6.attention.attention.value\n",
      "122 vision_model.encoder.layer.6.attention.attention.dropout\n",
      "123 vision_model.encoder.layer.6.attention.output\n",
      "124 vision_model.encoder.layer.6.attention.output.dense\n",
      "125 vision_model.encoder.layer.6.attention.output.dropout\n",
      "126 vision_model.encoder.layer.6.intermediate\n",
      "127 vision_model.encoder.layer.6.intermediate.dense\n",
      "128 vision_model.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "129 vision_model.encoder.layer.6.output\n",
      "130 vision_model.encoder.layer.6.output.dense\n",
      "131 vision_model.encoder.layer.6.output.dropout\n",
      "132 vision_model.encoder.layer.6.layernorm_before\n",
      "133 vision_model.encoder.layer.6.layernorm_after\n",
      "134 vision_model.encoder.layer.7\n",
      "135 vision_model.encoder.layer.7.attention\n",
      "136 vision_model.encoder.layer.7.attention.attention\n",
      "137 vision_model.encoder.layer.7.attention.attention.query\n",
      "138 vision_model.encoder.layer.7.attention.attention.key\n",
      "139 vision_model.encoder.layer.7.attention.attention.value\n",
      "140 vision_model.encoder.layer.7.attention.attention.dropout\n",
      "141 vision_model.encoder.layer.7.attention.output\n",
      "142 vision_model.encoder.layer.7.attention.output.dense\n",
      "143 vision_model.encoder.layer.7.attention.output.dropout\n",
      "144 vision_model.encoder.layer.7.intermediate\n",
      "145 vision_model.encoder.layer.7.intermediate.dense\n",
      "146 vision_model.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "147 vision_model.encoder.layer.7.output\n",
      "148 vision_model.encoder.layer.7.output.dense\n",
      "149 vision_model.encoder.layer.7.output.dropout\n",
      "150 vision_model.encoder.layer.7.layernorm_before\n",
      "151 vision_model.encoder.layer.7.layernorm_after\n",
      "152 vision_model.encoder.layer.8\n",
      "153 vision_model.encoder.layer.8.attention\n",
      "154 vision_model.encoder.layer.8.attention.attention\n",
      "155 vision_model.encoder.layer.8.attention.attention.query\n",
      "156 vision_model.encoder.layer.8.attention.attention.key\n",
      "157 vision_model.encoder.layer.8.attention.attention.value\n",
      "158 vision_model.encoder.layer.8.attention.attention.dropout\n",
      "159 vision_model.encoder.layer.8.attention.output\n",
      "160 vision_model.encoder.layer.8.attention.output.dense\n",
      "161 vision_model.encoder.layer.8.attention.output.dropout\n",
      "162 vision_model.encoder.layer.8.intermediate\n",
      "163 vision_model.encoder.layer.8.intermediate.dense\n",
      "164 vision_model.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "165 vision_model.encoder.layer.8.output\n",
      "166 vision_model.encoder.layer.8.output.dense\n",
      "167 vision_model.encoder.layer.8.output.dropout\n",
      "168 vision_model.encoder.layer.8.layernorm_before\n",
      "169 vision_model.encoder.layer.8.layernorm_after\n",
      "170 vision_model.encoder.layer.9\n",
      "171 vision_model.encoder.layer.9.attention\n",
      "172 vision_model.encoder.layer.9.attention.attention\n",
      "173 vision_model.encoder.layer.9.attention.attention.query\n",
      "174 vision_model.encoder.layer.9.attention.attention.key\n",
      "175 vision_model.encoder.layer.9.attention.attention.value\n",
      "176 vision_model.encoder.layer.9.attention.attention.dropout\n",
      "177 vision_model.encoder.layer.9.attention.output\n",
      "178 vision_model.encoder.layer.9.attention.output.dense\n",
      "179 vision_model.encoder.layer.9.attention.output.dropout\n",
      "180 vision_model.encoder.layer.9.intermediate\n",
      "181 vision_model.encoder.layer.9.intermediate.dense\n",
      "182 vision_model.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "183 vision_model.encoder.layer.9.output\n",
      "184 vision_model.encoder.layer.9.output.dense\n",
      "185 vision_model.encoder.layer.9.output.dropout\n",
      "186 vision_model.encoder.layer.9.layernorm_before\n",
      "187 vision_model.encoder.layer.9.layernorm_after\n",
      "188 vision_model.encoder.layer.10\n",
      "189 vision_model.encoder.layer.10.attention\n",
      "190 vision_model.encoder.layer.10.attention.attention\n",
      "191 vision_model.encoder.layer.10.attention.attention.query\n",
      "192 vision_model.encoder.layer.10.attention.attention.key\n",
      "193 vision_model.encoder.layer.10.attention.attention.value\n",
      "194 vision_model.encoder.layer.10.attention.attention.dropout\n",
      "195 vision_model.encoder.layer.10.attention.output\n",
      "196 vision_model.encoder.layer.10.attention.output.dense\n",
      "197 vision_model.encoder.layer.10.attention.output.dropout\n",
      "198 vision_model.encoder.layer.10.intermediate\n",
      "199 vision_model.encoder.layer.10.intermediate.dense\n",
      "200 vision_model.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "201 vision_model.encoder.layer.10.output\n",
      "202 vision_model.encoder.layer.10.output.dense\n",
      "203 vision_model.encoder.layer.10.output.dropout\n",
      "204 vision_model.encoder.layer.10.layernorm_before\n",
      "205 vision_model.encoder.layer.10.layernorm_after\n",
      "206 vision_model.encoder.layer.11\n",
      "207 vision_model.encoder.layer.11.attention\n",
      "208 vision_model.encoder.layer.11.attention.attention\n",
      "209 vision_model.encoder.layer.11.attention.attention.query\n",
      "210 vision_model.encoder.layer.11.attention.attention.key\n",
      "211 vision_model.encoder.layer.11.attention.attention.value\n",
      "212 vision_model.encoder.layer.11.attention.attention.dropout\n",
      "213 vision_model.encoder.layer.11.attention.output\n",
      "214 vision_model.encoder.layer.11.attention.output.dense\n",
      "215 vision_model.encoder.layer.11.attention.output.dropout\n",
      "216 vision_model.encoder.layer.11.intermediate\n",
      "217 vision_model.encoder.layer.11.intermediate.dense\n",
      "218 vision_model.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "219 vision_model.encoder.layer.11.output\n",
      "220 vision_model.encoder.layer.11.output.dense\n",
      "221 vision_model.encoder.layer.11.output.dropout\n",
      "222 vision_model.encoder.layer.11.layernorm_before\n",
      "223 vision_model.encoder.layer.11.layernorm_after\n",
      "224 vision_model.layernorm\n",
      "225 vision_model.pooler\n",
      "226 vision_model.pooler.dense\n",
      "227 vision_model.pooler.activation\n",
      "228 text_model\n",
      "229 text_model.embeddings\n",
      "230 text_model.embeddings.word_embeddings\n",
      "231 text_model.embeddings.position_embeddings\n",
      "232 text_model.embeddings.token_type_embeddings\n",
      "233 text_model.embeddings.LayerNorm\n",
      "234 text_model.embeddings.dropout\n",
      "235 text_model.encoder\n",
      "236 text_model.encoder.layer\n",
      "237 text_model.encoder.layer.0\n",
      "238 text_model.encoder.layer.0.attention\n",
      "239 text_model.encoder.layer.0.attention.self\n",
      "240 text_model.encoder.layer.0.attention.self.query\n",
      "241 text_model.encoder.layer.0.attention.self.key\n",
      "242 text_model.encoder.layer.0.attention.self.value\n",
      "243 text_model.encoder.layer.0.attention.self.dropout\n",
      "244 text_model.encoder.layer.0.attention.output\n",
      "245 text_model.encoder.layer.0.attention.output.dense\n",
      "246 text_model.encoder.layer.0.attention.output.LayerNorm\n",
      "247 text_model.encoder.layer.0.attention.output.dropout\n",
      "248 text_model.encoder.layer.0.intermediate\n",
      "249 text_model.encoder.layer.0.intermediate.dense\n",
      "250 text_model.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "251 text_model.encoder.layer.0.output\n",
      "252 text_model.encoder.layer.0.output.dense\n",
      "253 text_model.encoder.layer.0.output.LayerNorm\n",
      "254 text_model.encoder.layer.0.output.dropout\n",
      "255 text_model.encoder.layer.1\n",
      "256 text_model.encoder.layer.1.attention\n",
      "257 text_model.encoder.layer.1.attention.self\n",
      "258 text_model.encoder.layer.1.attention.self.query\n",
      "259 text_model.encoder.layer.1.attention.self.key\n",
      "260 text_model.encoder.layer.1.attention.self.value\n",
      "261 text_model.encoder.layer.1.attention.self.dropout\n",
      "262 text_model.encoder.layer.1.attention.output\n",
      "263 text_model.encoder.layer.1.attention.output.dense\n",
      "264 text_model.encoder.layer.1.attention.output.LayerNorm\n",
      "265 text_model.encoder.layer.1.attention.output.dropout\n",
      "266 text_model.encoder.layer.1.intermediate\n",
      "267 text_model.encoder.layer.1.intermediate.dense\n",
      "268 text_model.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "269 text_model.encoder.layer.1.output\n",
      "270 text_model.encoder.layer.1.output.dense\n",
      "271 text_model.encoder.layer.1.output.LayerNorm\n",
      "272 text_model.encoder.layer.1.output.dropout\n",
      "273 text_model.encoder.layer.2\n",
      "274 text_model.encoder.layer.2.attention\n",
      "275 text_model.encoder.layer.2.attention.self\n",
      "276 text_model.encoder.layer.2.attention.self.query\n",
      "277 text_model.encoder.layer.2.attention.self.key\n",
      "278 text_model.encoder.layer.2.attention.self.value\n",
      "279 text_model.encoder.layer.2.attention.self.dropout\n",
      "280 text_model.encoder.layer.2.attention.output\n",
      "281 text_model.encoder.layer.2.attention.output.dense\n",
      "282 text_model.encoder.layer.2.attention.output.LayerNorm\n",
      "283 text_model.encoder.layer.2.attention.output.dropout\n",
      "284 text_model.encoder.layer.2.intermediate\n",
      "285 text_model.encoder.layer.2.intermediate.dense\n",
      "286 text_model.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "287 text_model.encoder.layer.2.output\n",
      "288 text_model.encoder.layer.2.output.dense\n",
      "289 text_model.encoder.layer.2.output.LayerNorm\n",
      "290 text_model.encoder.layer.2.output.dropout\n",
      "291 text_model.encoder.layer.3\n",
      "292 text_model.encoder.layer.3.attention\n",
      "293 text_model.encoder.layer.3.attention.self\n",
      "294 text_model.encoder.layer.3.attention.self.query\n",
      "295 text_model.encoder.layer.3.attention.self.key\n",
      "296 text_model.encoder.layer.3.attention.self.value\n",
      "297 text_model.encoder.layer.3.attention.self.dropout\n",
      "298 text_model.encoder.layer.3.attention.output\n",
      "299 text_model.encoder.layer.3.attention.output.dense\n",
      "300 text_model.encoder.layer.3.attention.output.LayerNorm\n",
      "301 text_model.encoder.layer.3.attention.output.dropout\n",
      "302 text_model.encoder.layer.3.intermediate\n",
      "303 text_model.encoder.layer.3.intermediate.dense\n",
      "304 text_model.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "305 text_model.encoder.layer.3.output\n",
      "306 text_model.encoder.layer.3.output.dense\n",
      "307 text_model.encoder.layer.3.output.LayerNorm\n",
      "308 text_model.encoder.layer.3.output.dropout\n",
      "309 text_model.encoder.layer.4\n",
      "310 text_model.encoder.layer.4.attention\n",
      "311 text_model.encoder.layer.4.attention.self\n",
      "312 text_model.encoder.layer.4.attention.self.query\n",
      "313 text_model.encoder.layer.4.attention.self.key\n",
      "314 text_model.encoder.layer.4.attention.self.value\n",
      "315 text_model.encoder.layer.4.attention.self.dropout\n",
      "316 text_model.encoder.layer.4.attention.output\n",
      "317 text_model.encoder.layer.4.attention.output.dense\n",
      "318 text_model.encoder.layer.4.attention.output.LayerNorm\n",
      "319 text_model.encoder.layer.4.attention.output.dropout\n",
      "320 text_model.encoder.layer.4.intermediate\n",
      "321 text_model.encoder.layer.4.intermediate.dense\n",
      "322 text_model.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "323 text_model.encoder.layer.4.output\n",
      "324 text_model.encoder.layer.4.output.dense\n",
      "325 text_model.encoder.layer.4.output.LayerNorm\n",
      "326 text_model.encoder.layer.4.output.dropout\n",
      "327 text_model.encoder.layer.5\n",
      "328 text_model.encoder.layer.5.attention\n",
      "329 text_model.encoder.layer.5.attention.self\n",
      "330 text_model.encoder.layer.5.attention.self.query\n",
      "331 text_model.encoder.layer.5.attention.self.key\n",
      "332 text_model.encoder.layer.5.attention.self.value\n",
      "333 text_model.encoder.layer.5.attention.self.dropout\n",
      "334 text_model.encoder.layer.5.attention.output\n",
      "335 text_model.encoder.layer.5.attention.output.dense\n",
      "336 text_model.encoder.layer.5.attention.output.LayerNorm\n",
      "337 text_model.encoder.layer.5.attention.output.dropout\n",
      "338 text_model.encoder.layer.5.intermediate\n",
      "339 text_model.encoder.layer.5.intermediate.dense\n",
      "340 text_model.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "341 text_model.encoder.layer.5.output\n",
      "342 text_model.encoder.layer.5.output.dense\n",
      "343 text_model.encoder.layer.5.output.LayerNorm\n",
      "344 text_model.encoder.layer.5.output.dropout\n",
      "345 text_model.encoder.layer.6\n",
      "346 text_model.encoder.layer.6.attention\n",
      "347 text_model.encoder.layer.6.attention.self\n",
      "348 text_model.encoder.layer.6.attention.self.query\n",
      "349 text_model.encoder.layer.6.attention.self.key\n",
      "350 text_model.encoder.layer.6.attention.self.value\n",
      "351 text_model.encoder.layer.6.attention.self.dropout\n",
      "352 text_model.encoder.layer.6.attention.output\n",
      "353 text_model.encoder.layer.6.attention.output.dense\n",
      "354 text_model.encoder.layer.6.attention.output.LayerNorm\n",
      "355 text_model.encoder.layer.6.attention.output.dropout\n",
      "356 text_model.encoder.layer.6.intermediate\n",
      "357 text_model.encoder.layer.6.intermediate.dense\n",
      "358 text_model.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "359 text_model.encoder.layer.6.output\n",
      "360 text_model.encoder.layer.6.output.dense\n",
      "361 text_model.encoder.layer.6.output.LayerNorm\n",
      "362 text_model.encoder.layer.6.output.dropout\n",
      "363 text_model.encoder.layer.7\n",
      "364 text_model.encoder.layer.7.attention\n",
      "365 text_model.encoder.layer.7.attention.self\n",
      "366 text_model.encoder.layer.7.attention.self.query\n",
      "367 text_model.encoder.layer.7.attention.self.key\n",
      "368 text_model.encoder.layer.7.attention.self.value\n",
      "369 text_model.encoder.layer.7.attention.self.dropout\n",
      "370 text_model.encoder.layer.7.attention.output\n",
      "371 text_model.encoder.layer.7.attention.output.dense\n",
      "372 text_model.encoder.layer.7.attention.output.LayerNorm\n",
      "373 text_model.encoder.layer.7.attention.output.dropout\n",
      "374 text_model.encoder.layer.7.intermediate\n",
      "375 text_model.encoder.layer.7.intermediate.dense\n",
      "376 text_model.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "377 text_model.encoder.layer.7.output\n",
      "378 text_model.encoder.layer.7.output.dense\n",
      "379 text_model.encoder.layer.7.output.LayerNorm\n",
      "380 text_model.encoder.layer.7.output.dropout\n",
      "381 text_model.encoder.layer.8\n",
      "382 text_model.encoder.layer.8.attention\n",
      "383 text_model.encoder.layer.8.attention.self\n",
      "384 text_model.encoder.layer.8.attention.self.query\n",
      "385 text_model.encoder.layer.8.attention.self.key\n",
      "386 text_model.encoder.layer.8.attention.self.value\n",
      "387 text_model.encoder.layer.8.attention.self.dropout\n",
      "388 text_model.encoder.layer.8.attention.output\n",
      "389 text_model.encoder.layer.8.attention.output.dense\n",
      "390 text_model.encoder.layer.8.attention.output.LayerNorm\n",
      "391 text_model.encoder.layer.8.attention.output.dropout\n",
      "392 text_model.encoder.layer.8.intermediate\n",
      "393 text_model.encoder.layer.8.intermediate.dense\n",
      "394 text_model.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "395 text_model.encoder.layer.8.output\n",
      "396 text_model.encoder.layer.8.output.dense\n",
      "397 text_model.encoder.layer.8.output.LayerNorm\n",
      "398 text_model.encoder.layer.8.output.dropout\n",
      "399 text_model.encoder.layer.9\n",
      "400 text_model.encoder.layer.9.attention\n",
      "401 text_model.encoder.layer.9.attention.self\n",
      "402 text_model.encoder.layer.9.attention.self.query\n",
      "403 text_model.encoder.layer.9.attention.self.key\n",
      "404 text_model.encoder.layer.9.attention.self.value\n",
      "405 text_model.encoder.layer.9.attention.self.dropout\n",
      "406 text_model.encoder.layer.9.attention.output\n",
      "407 text_model.encoder.layer.9.attention.output.dense\n",
      "408 text_model.encoder.layer.9.attention.output.LayerNorm\n",
      "409 text_model.encoder.layer.9.attention.output.dropout\n",
      "410 text_model.encoder.layer.9.intermediate\n",
      "411 text_model.encoder.layer.9.intermediate.dense\n",
      "412 text_model.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "413 text_model.encoder.layer.9.output\n",
      "414 text_model.encoder.layer.9.output.dense\n",
      "415 text_model.encoder.layer.9.output.LayerNorm\n",
      "416 text_model.encoder.layer.9.output.dropout\n",
      "417 text_model.encoder.layer.10\n",
      "418 text_model.encoder.layer.10.attention\n",
      "419 text_model.encoder.layer.10.attention.self\n",
      "420 text_model.encoder.layer.10.attention.self.query\n",
      "421 text_model.encoder.layer.10.attention.self.key\n",
      "422 text_model.encoder.layer.10.attention.self.value\n",
      "423 text_model.encoder.layer.10.attention.self.dropout\n",
      "424 text_model.encoder.layer.10.attention.output\n",
      "425 text_model.encoder.layer.10.attention.output.dense\n",
      "426 text_model.encoder.layer.10.attention.output.LayerNorm\n",
      "427 text_model.encoder.layer.10.attention.output.dropout\n",
      "428 text_model.encoder.layer.10.intermediate\n",
      "429 text_model.encoder.layer.10.intermediate.dense\n",
      "430 text_model.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "431 text_model.encoder.layer.10.output\n",
      "432 text_model.encoder.layer.10.output.dense\n",
      "433 text_model.encoder.layer.10.output.LayerNorm\n",
      "434 text_model.encoder.layer.10.output.dropout\n",
      "435 text_model.encoder.layer.11\n",
      "436 text_model.encoder.layer.11.attention\n",
      "437 text_model.encoder.layer.11.attention.self\n",
      "438 text_model.encoder.layer.11.attention.self.query\n",
      "439 text_model.encoder.layer.11.attention.self.key\n",
      "440 text_model.encoder.layer.11.attention.self.value\n",
      "441 text_model.encoder.layer.11.attention.self.dropout\n",
      "442 text_model.encoder.layer.11.attention.output\n",
      "443 text_model.encoder.layer.11.attention.output.dense\n",
      "444 text_model.encoder.layer.11.attention.output.LayerNorm\n",
      "445 text_model.encoder.layer.11.attention.output.dropout\n",
      "446 text_model.encoder.layer.11.intermediate\n",
      "447 text_model.encoder.layer.11.intermediate.dense\n",
      "448 text_model.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "449 text_model.encoder.layer.11.output\n",
      "450 text_model.encoder.layer.11.output.dense\n",
      "451 text_model.encoder.layer.11.output.LayerNorm\n",
      "452 text_model.encoder.layer.11.output.dropout\n",
      "453 text_model.pooler\n",
      "454 text_model.pooler.dense\n",
      "455 text_model.pooler.activation\n",
      "456 visual_projection\n",
      "457 text_projection\n"
     ]
    }
   ],
   "source": [
    "for i, (name, layer) in enumerate(model_clip.named_modules()):\n",
    "    print(i, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [f\"vision_model.encoder.layer.{i}\" for i in range(12)] + [\"vision_model.pooler\", \"visual_projection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cka = CKA(model_clip, model_clip_itm,\n",
    "    model1_name = \"CLIP\", model2_name = \"CLIP + ITM\",\n",
    "    model1_layers=layers, model2_layers=layers,\n",
    "    device=\"cuda:3\")\n",
    "cka.model1 = lambda x: model_clip.get_image_features(pixel_values=x)\n",
    "cka.model2 = lambda x: model_clip_itm.get_image_features(pixel_values=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = cfg.data\n",
    "data_module = MyDataModule(data_config, processor, augmentation=RandAugment(), num_views=2)\n",
    "test_dataloaders = data_module.get_test_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dataloader = test_dataloaders[\"CIFAR10\"][\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del batch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(cifar10_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
       "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
       "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
       "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
       "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
       "        8, 3, 1, 2, 8, 0, 8, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = model_clip_simclr_itm.get_image_features(pixel_values=batch[0].cuda(\"cuda:3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/torch_cka/cka.py:145: UserWarning: Dataloader for Model 2 is not given. Using the same dataloader for both models.\n",
      "  warn(\"Dataloader for Model 2 is not given. Using the same dataloader for both models.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Comparing features |:   0%|          | 0/79 [00:00<?, ?it/s]ERROR:tornado.general:SEND Error: Host unreachable\n",
      "| Comparing features |:   0%|          | 0/79 [07:08<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "cka.compare(cifar10_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
