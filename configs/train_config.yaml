#resume_checkpoint: '/home/phisch/multimodal/models/cliplike/ckpts/5m917c9w/last.ckpt'

wandb:
  #name: 
  tags :
    - 'clip'
    - 'cifar10'
    - 'coco'
    - 'vg'
    - 'cc_full'
    - 'itmloss'
    - 'simclr'

  #group: 'debug'
  group: 'test'
  #group: 'run'
  #group: 'clip'
  project : 'multimodal'
  dir : './wandb/'
  ### checkpoint args
  #id: 5m917c9w

save_dir: './models/cliplike/'

lightning:
  seed : 69
  trainer:
    fast_dev_run: false
    #overfit_batches: 5 #5
    val_check_interval: 0.3
    log_every_n_steps: 4 #5 #10
    devices: 2  #[0,1] #[3] #[0, 1]
    accelerator: 'auto'
    strategy: 'ddp' #'ddp_find_unused_parameters_true'
    deterministic: 'warn'
    precision: '16-mixed'
    gradient_clip_algorithm: "norm"
    gradient_clip_val: 1.0
    max_epochs: 100
    # num_nodes : 4   
    static_graph: True
  model_checkpoint_callback:
    every_n_epochs: 5
    save_last: True
    save_top_k: 2
  cifar_linear_probe_callback:
    logging_interval: 'epoch'
    log_every: 3
    confusion_matrix: True
    verbose: False
    max_epochs: 400
  caltech101_linear_probe_callback:
    logging_interval: 'epoch'
    log_every: 3
    confusion_matrix: True
    verbose: False
    max_epochs: 400



gradient_checkpointing: True

dataloader:
  train:
    shuffle : True
    batch_size : 2048  #896 #960 #1024 #256 #180
    #batch_size : 192
    num_workers : 2
    persistent_workers : True
    pin_memory: True

  coco_val:
    shuffle : False
    #batch_size : 
    batch_size : 2048 #896 #960 #1024 #256 #180
    num_workers : 2
    persistent_workers : True
    pin_memory: True
  
  cifar10_val:
    batch_size: 512
    shuffle: False
    num_workers: 2
    #persistent_workers: True
    pin_memory: True

  caltech101_val:
    batch_size: 512
    shuffle: False
    num_workers: 2
    #persistent_workers: True
    pin_memory: True


loss:
  losses:
    - 'contrastive'
    - 'image_text_matching'
    - 'SimCLR'
  contrastive:
  #  temperature : 1.
  image_text_matching:
    arg1: ''


optimizer:
  name : "AdamW"
  lr: 2e-04
  kwargs:
    weight_decay : 0.1
    betas : [0.9,0.95]



# scheduler:
#   name:
#     - 'CosineWarmup'
#     #- 'CosineWarmupHardRestarts'
#   kwargs:
#     #T_max : 1000
#     num_warmup_steps: 'epoch'
#     num_training_steps: 'all' #20000
#     #num_cycles: 10
#     initial_lr: 1e-08
#   interval: "step"



# scheduler:
#   name: SequentialLR
#   monitor: 'loss-val/dataloader_idx_0'
#   interval: 'step'
#   kwargs: 
#     milestones: [100]
#   sub_schedulers:
#     - name: 'CosineAnnealingWarmRestarts'
#       kwargs:
#         T_0: 100
#     - name: 'ExponentialLR'
#       kwargs:
#         gamma: 0.95
#         verbose: true


# scheduler:
#   name: CyclicLR
#   monitor: 'loss-val/dataloader_idx_0'
#   interval: 'step'
#   kwargs:
#     base_lr: 0.0003
#     max_lr: 0.0004


# scheduler:
#   name: OneCycleLR
#   monitor: 'loss-val/dataloader_idx_0'
#   interval: 'step'
#   kwargs:
#     max_lr: 0.0004
#     steps_per_epoch: 121
#     epochs: 40
  
scheduler:
  name: CosineWarmup
  monitor: 'loss-val/dataloader_idx_0'
  interval: 'step'
  kwargs:
    initial_lr: 1e-08
    num_warmup_steps: 'epoch'
    num_training_steps: 'all'

model:
  #image_encoder_name : 'openai/clip-vit-base-patch32'
  #text_encoder_name : 'openai/clip-vit-base-patch32'
  image_encoder_name : 'google/vit-base-patch16-224'
  text_encoder_name : 'google-bert/bert-base-uncased'
  tokenizer :
    use_fast: False
  


dataset:
  train:
    - 'coco'
    - 'vg'
    - 'cc3m'
  val:
    - 'coco_val'
    - 'cifar10'
    - 'caltech101'
  transforms:
    RandAugment:
      num_ops: 3
      magnitude: 8
  max_seq_length: 72
  coco:
    data_dir : '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/shared/datasets/COCO'
    split_train : '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/bhavin/students/phillipscholl/multimodal/my_datasets/coco_karpathy_train.json'
    split_val : '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/bhavin/students/phillipscholl/multimodal/my_datasets/coco_karpathy_val.json'
    split_test : '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/bhavin/students/phillipscholl/multimodal/my_datasets/coco_karpathy_test.json'
  vg:
    data_dir: '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/shared/datasets/VG_Bhavin/VG'
  cc3m:
    data_dir: '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/shared/datasets/CC3m/h5'
  cifar10:
    root: '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/shared/datasets/cifar10'
    download: false
  caltech101 :
    root: '/pfss/mlde/workspaces/mlde_wsp_PI_Roig/shared/datasets/cifar10'
    download: false
  use_subset: #for train
    value: true
    subset_fraction: 0.4
  use_subset_probe: #for cifar and caltech only
    value: false
    subset_fraction: 0.0
    

zeroshot:
  templates:
    - 'a photo of a {}.'
    - 'a blurry photo of a {}.'
    - 'a black and white photo of a {}.'
    - 'a low contrast photo of a {}.'
    - 'a high contrast photo of a {}.'
    - 'a bad photo of a {}.'
    - 'a good photo of a {}.'
    - 'a photo of a small {}.'
    - 'a photo of a big {}.'
    - 'a photo of the {}.'
    - 'a blurry photo of the {}.'
    - 'a black and white photo of the {}.'
    - 'a low contrast photo of the {}.'
    - 'a high contrast photo of the {}.'
    - 'a bad photo of the {}.'
    - 'a good photo of the {}.'
    - 'a photo of the small {}.'
    - 'a photo of the big {}.'

# scheduler:
#   name: SequentialLR
#   kwargs: 
#     milestones: [2]
#   sub_schedulers:
#     - name: 'ReduceROnPlateau'
#       kwargs:
#         num_warmup_steps: 'epoch'
#     - name: 'CosineWarmupHardRestarts'
#       kwargs:
#             T_max: 1000
