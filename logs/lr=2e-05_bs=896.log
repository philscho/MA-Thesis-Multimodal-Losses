nohup: ignoring input
Seed set to 69
wandb: Currently logged in as: philscho (philscho-org). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in ./wandb/run-20240709_101258-9mefpejr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lr=2e-05_bs=896
wandb: ‚≠êÔ∏è View project at https://wandb.ai/philscho-org/multimodal
wandb: üöÄ View run at https://wandb.ai/philscho-org/multimodal/runs/9mefpejr
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Seed set to 69
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
[rank: 2] Seed set to 69
[rank: 1] Seed set to 69
[rank: 3] Seed set to 69
[rank: 2] Seed set to 69
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
[rank: 1] Seed set to 69
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[rank: 3] Seed set to 69
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Loading `train_dataloader` to estimate number of stepping batches.

  | Name             | Type                           | Params
--------------------------------------------------------------------
0 | model            | VisionTextDualEncoderModel     | 196 M 
1 | contrastive_loss | ContrastiveLossWithTemperature | 1     
--------------------------------------------------------------------
196 M     Trainable params
1         Non-trainable params
196 M     Total params
786.632   Total estimated model params size (MB)
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  0.21it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:43<00:00,  0.05it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]        Sanity Checking DataLoader 1:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 97.18it/s]Sanity Checking DataLoader 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 13.78it/s]                                                                           /home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('cifar10-accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/184 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/184 [00:00<?, ?it/s] Epoch 0:   1%|          | 1/184 [00:42<2:08:42,  0.02it/s]Epoch 0:   1%|          | 1/184 [00:42<2:08:42,  0.02it/s, v_num=pejr, loss-train=8.250]Epoch 0:   1%|          | 2/184 [01:04<1:38:10,  0.03it/s, v_num=pejr, loss-train=8.250]Epoch 0:   1%|          | 2/184 [01:04<1:38:10,  0.03it/s, v_num=pejr, loss-train=8.250]Epoch 0:   2%|‚ñè         | 3/184 [01:27<1:28:21,  0.03it/s, v_num=pejr, loss-train=8.250]Epoch 0:   2%|‚ñè         | 3/184 [01:27<1:28:22,  0.03it/s, v_num=pejr, loss-train=8.240]Epoch 0:   2%|‚ñè         | 4/184 [01:49<1:22:22,  0.04it/s, v_num=pejr, loss-train=8.240]Epoch 0:   2%|‚ñè         | 4/184 [01:49<1:22:22,  0.04it/s, v_num=pejr, loss-train=8.240]Epoch 0:   3%|‚ñé         | 5/184 [02:12<1:19:18,  0.04it/s, v_num=pejr, loss-train=8.240]Epoch 0:   3%|‚ñé         | 5/184 [02:12<1:19:19,  0.04it/s, v_num=pejr, loss-train=8.230]Epoch 0:   3%|‚ñé         | 6/184 [02:35<1:16:50,  0.04it/s, v_num=pejr, loss-train=8.230]Epoch 0:   3%|‚ñé         | 6/184 [02:35<1:16:50,  0.04it/s, v_num=pejr, loss-train=8.240]Epoch 0:   4%|‚ñç         | 7/184 [03:01<1:16:27,  0.04it/s, v_num=pejr, loss-train=8.240]Epoch 0:   4%|‚ñç         | 7/184 [03:01<1:16:27,  0.04it/s, v_num=pejr, loss-train=8.240]Epoch 0:   4%|‚ñç         | 8/184 [03:25<1:15:30,  0.04it/s, v_num=pejr, loss-train=8.240]Epoch 0:   4%|‚ñç         | 8/184 [03:25<1:15:30,  0.04it/s, v_num=pejr, loss-train=8.230]Traceback (most recent call last):
  File "/home/phisch/multimodal/train_model_coco_dualenc_new.py", line 558, in <module>
    main(config)
  File "/home/phisch/multimodal/train_model_coco_dualenc_new.py", line 531, in main
    trainer.fit(net, train_dataloaders=train_dataloader, val_dataloaders=val_dataloaders)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/plugins/precision/amp.py", line 80, in optimizer_step
    closure_result = closure()
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 138, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 240, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/plugins/precision/precision.py", line 72, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/lightning/pytorch/core/module.py", line 1090, in backward
    loss.backward(*args, **kwargs)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 157, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.01 GiB (GPU 2; 39.59 GiB total capacity; 18.39 GiB already allocated; 698.94 MiB free; 23.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
