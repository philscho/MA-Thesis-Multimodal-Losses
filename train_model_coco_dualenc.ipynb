{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2 as transforms\n",
    "#from src.data import MultiModalH5PyDataset\n",
    "from torchmultimodal.modules.losses.contrastive_loss_with_temperature import ContrastiveLossWithTemperature\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import lightning as pl\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.utilities import rank_zero_only\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    BertConfig,\n",
    "    ViTConfig,\n",
    "    VisionTextDualEncoderConfig\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, split_file, image_dir, processor=None, transform=None):\n",
    "        self.split = load_json(split_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.split[idx]\n",
    "        #img_filename = sample['filename']\n",
    "        img_filenames = sample['filename']\n",
    "        img_filename = img_filenames[0] if isinstance(img_filenames, list) else img_filenames\n",
    "        img_path = os.path.join(self.image_dir, img_filename)       \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations if provided\n",
    "        if self.processor:\n",
    "            img = torch.squeeze(self.processor(images=image, return_tensors=\"pt\").pixel_values)\n",
    "        else:\n",
    "            img = image\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        #captions = sample['sentences']\n",
    "        #caption = captions[0]\n",
    "        caption = sample['sentences 0']\n",
    "        \n",
    "        return img, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(dataset, data_dir, processor):\n",
    "\n",
    "    image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=.5, contrast=.2, saturation=.3, hue=.2),\n",
    "            transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_dataset = CustomDataset(\n",
    "        'coco_karpathy_train.json', \n",
    "        os.path.join(data_dir, \"train2014\"), \n",
    "        processor,\n",
    "        # image_transforms\n",
    "    )\n",
    "\n",
    "    val_dataset = CustomDataset(\n",
    "        'coco_karpathy_val.json', \n",
    "        os.path.join(data_dir, \"val2014\"), \n",
    "        processor,\n",
    "        # image_transforms\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_dataset, val_dataset, **kwargs):\n",
    "\n",
    "    # def collate_fn(samples):\n",
    "    #     pixel_values = torch.stack([img for (img, text) in samples])\n",
    "    #     text = [text for (img, text) in samples]\n",
    "    #     return {\"pixel_values\": pixel_values, \"text\": text}\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, **kwargs)\n",
    "    val_dataloader = DataLoader(val_dataset, **kwargs)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(image_encoder_name, text_encoder_name):\n",
    "    config_vision = ViTConfig()\n",
    "    config_text = BertConfig()\n",
    "    config_model = VisionTextDualEncoderConfig.from_vision_text_configs(vision_config=config_vision, text_config=config_text)\n",
    "    \n",
    "    model = VisionTextDualEncoderModel(config_model)\n",
    "\n",
    "    image_processor = AutoImageProcessor.from_pretrained(image_encoder_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text_encoder_name, use_fast=False)\n",
    "    processor = VisionTextDualEncoderProcessor(image_processor=image_processor, tokenizer=tokenizer)\n",
    "\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMML(pl.LightningModule):\n",
    "    # def __init__(self, image_encoder, text_encoder,tokenizer,temperature,learning_rate):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        processor,\n",
    "        loss_cfg,\n",
    "        optimizer_cfg,\n",
    "        scheduler_cfg,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.image_encoder = image_encoder\n",
    "        # self.text_encoder = text_encoder\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "\n",
    "        self.loss_cfg = loss_cfg\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.scheduler_cfg = scheduler_cfg\n",
    "\n",
    "        self.contrastive_loss = ContrastiveLossWithTemperature()\n",
    "\n",
    "        # self.temperature = temperature\n",
    "        # self.learning_rate = learning_rate\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\", \"image_encoder\", \"text_encoder\", \"tokenizer\"])\n",
    "    \n",
    "    # def encode_image(self, imgs):\n",
    "    #     image_out = self.image_encoder(imgs)\n",
    "    #     return image_out\n",
    "\n",
    "    # def encode_text(self, txt):\n",
    "    #     tok_y = self.tokenizer(txt, padding=True, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "    #     text_out = self.text_encoder(**tok_y)\n",
    "    #     return text_out\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self.processor(text=text, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        return tokens\n",
    "    \n",
    "    def forward(self, images, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return self.model(**tokens, pixel_values=images)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, text = batch\n",
    "\n",
    "        # image_out = self.encode_image(x)[\"image_embeds\"]\n",
    "        # text_out = self.encode_text(y)[\"text_embeds\"]\n",
    "\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        outputs = self.model(**tokens, pixel_values=images)\n",
    "        # outputs = self.model(\n",
    "        #     input_ids=inputs.input_ids,\n",
    "        #     attention_mask=inputs.attention_mask,\n",
    "        #     pixel_values=inputs.pixel_values,\n",
    "        #     return_loss=True\n",
    "        # )\n",
    "\n",
    "        image_out = torch.nn.functional.normalize(outputs.image_embeds, dim=-1)\n",
    "        text_out = torch.nn.functional.normalize(outputs.text_embeds, dim=-1)\n",
    "\n",
    "        # loss, logits_per_image = outputs.loss, outputs.logits_per_image\n",
    "        # self.log(\"loss/train\", loss.mean(), sync_dist=True)\n",
    "        # return loss.mean()\n",
    "\n",
    "        #if self.loss_cfg.name == \"contrastive_like_clip\":\n",
    "        #loss = clip_contrastive_loss(image_out, text_out, self.loss_cfg.temperature)\n",
    "        loss = self.contrastive_loss(image_out, text_out).mean()\n",
    "        self.log(\"loss/train\", loss, sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, text = batch\n",
    "\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        outputs = self.model(**tokens, pixel_values=images)\n",
    "        # outputs = self.model(\n",
    "        #     input_ids=inputs.input_ids,\n",
    "        #     attention_mask=inputs.attention_mask,\n",
    "        #     pixel_values=inputs.pixel_values,\n",
    "        #     return_loss=True\n",
    "        # )\n",
    "\n",
    "        image_out = torch.nn.functional.normalize(outputs.image_embeds, dim=-1)\n",
    "        text_out = torch.nn.functional.normalize(outputs.text_embeds, dim=-1)\n",
    "\n",
    "        # loss, logits_per_image = outputs.loss, outputs.logits_per_image\n",
    "        # self.log(\"loss/train\", loss.mean(), sync_dist=True)\n",
    "        # return loss.mean()\n",
    "    \n",
    "        #loss = clip_contrastive_loss(image_out, text_out, self.loss_cfg.temperature)\n",
    "        loss = self.contrastive_loss(image_out, text_out).mean()\n",
    "        self.log(\"loss/val\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "        \n",
    "        if self.optimizer_cfg.name == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), **self.optimizer_cfg.kwargs)\n",
    "        elif self.optimizer_cfg.name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), **self.optimizer_cfg.kwargs)\n",
    "        elif self.optimizer_cfg.name == \"AdamW\":\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), **self.optimizer_cfg.kwargs)\n",
    "        elif self.optimizer_cfg.name == \"Adagrad\":\n",
    "            optimizer = torch.optim.Adagrad(self.parameters(), **self.optimizer_cfg.kwargs)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Wrong optimizer name. Provided {self.optimizer_cfg.name} which doesn't exist\"\n",
    "            )\n",
    "\n",
    "        # check if scheduler is to be used\n",
    "        if self.scheduler_cfg.name is None:\n",
    "            print(\"No scheduler provided, using only optimizer\")\n",
    "            return optimizer\n",
    "\n",
    "        elif self.scheduler_cfg.name == \"CosineAnnealingLR\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, **self.scheduler_cfg.kwargs\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": scheduler,\n",
    "            }\n",
    "\n",
    "        elif self.scheduler_cfg.name == \"CosineAnnealingLRWarmRestarts\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLRWarmRestarts(\n",
    "                optimizer, **self.scheduler_cfg.kwargs\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": scheduler,\n",
    "            }\n",
    "\n",
    "        elif self.scheduler_cfg.name == \"ReduceROnPlateau\":\n",
    "            monitor_metric = self.scheduler_cfg.kwargs.pop(\"monitor\")\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceROnPlateau(\n",
    "                optimizer, **self.scheduler_cfg.kwargs\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": monitor_metric},\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Wrong scheduler name. Provided {self.scheduler_cfg.name} which doesn't exist\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor = get_models('google/vit-base-patch16-224', 'google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yerevann/coco-karpathy\")\n",
    "#train_dataset, val_dataset = get_datasets(dataset, '/home/data/COCOcaptions', processor)\n",
    "train_dataset, val_dataset = get_datasets(dataset, '/home/data/COCOcaptions', processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_dataloaders(train_dataset, val_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LitMML(\n",
    "        model,\n",
    "        processor,\n",
    "        loss_cfg='contrastive_like_clip',\n",
    "        optimizer_cfg=\"AdamW\",\n",
    "        scheduler_cfg=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x7f161c66d4f0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.3569, -0.3725, -0.3176,  ...,  0.0196, -0.0039,  0.4588],\n",
       "           [-0.3569, -0.3804, -0.3020,  ...,  0.0039,  0.0196,  0.4824],\n",
       "           [-0.3882, -0.3961, -0.3255,  ...,  0.0667, -0.0196,  0.0902],\n",
       "           ...,\n",
       "           [-0.6392, -0.6784, -0.6863,  ...,  0.2392,  0.2314,  0.2000],\n",
       "           [-0.6000, -0.6235, -0.6471,  ...,  0.2157,  0.2078,  0.2078],\n",
       "           [-0.5059, -0.5137, -0.5529,  ...,  0.2157,  0.1765,  0.1922]],\n",
       " \n",
       "          [[-0.6471, -0.6471, -0.6235,  ..., -0.2627, -0.2235,  0.1059],\n",
       "           [-0.6941, -0.6863, -0.6863,  ..., -0.2471, -0.1922,  0.1608],\n",
       "           [-0.6941, -0.6941, -0.7020,  ..., -0.1686, -0.2078, -0.1059],\n",
       "           ...,\n",
       "           [-0.8588, -0.8745, -0.8667,  ...,  0.3255,  0.2941,  0.2784],\n",
       "           [-0.8431, -0.8353, -0.8196,  ...,  0.2863,  0.2784,  0.2706],\n",
       "           [-0.7255, -0.7333, -0.7647,  ...,  0.2706,  0.2549,  0.2392]],\n",
       " \n",
       "          [[-0.7882, -0.7804, -0.7647,  ..., -0.2549, -0.2235,  0.0196],\n",
       "           [-0.8118, -0.8353, -0.7961,  ..., -0.2549, -0.1922,  0.0902],\n",
       "           [-0.8196, -0.7882, -0.8431,  ..., -0.2235, -0.2314, -0.1529],\n",
       "           ...,\n",
       "           [-0.8588, -0.8902, -0.8980,  ...,  0.9686,  0.9686,  0.9608],\n",
       "           [-0.8667, -0.8588, -0.8745,  ...,  0.9608,  0.9529,  0.9529],\n",
       "           [-0.8745, -0.8353, -0.8745,  ...,  0.9686,  0.9373,  0.9294]]],\n",
       " \n",
       " \n",
       "         [[[-0.1373, -0.1216, -0.1137,  ..., -0.1059, -0.0353, -0.0353],\n",
       "           [-0.1059, -0.0980, -0.0980,  ...,  0.1451,  0.2000,  0.2078],\n",
       "           [-0.1216, -0.1137, -0.1294,  ...,  0.3020,  0.3255,  0.3569],\n",
       "           ...,\n",
       "           [ 0.2941,  0.3255,  0.3647,  ..., -0.3725, -0.3882, -0.3961],\n",
       "           [ 0.3490,  0.3882,  0.4118,  ..., -0.3647, -0.3882, -0.3804],\n",
       "           [ 0.3804,  0.3961,  0.4118,  ..., -0.3725, -0.3882, -0.3804]],\n",
       " \n",
       "          [[-0.3882, -0.3647, -0.3647,  ..., -0.3098, -0.2863, -0.2941],\n",
       "           [-0.3569, -0.3412, -0.3412,  ..., -0.1529, -0.1137, -0.1059],\n",
       "           [-0.3725, -0.3647, -0.3804,  ...,  0.0118,  0.0353,  0.0431],\n",
       "           ...,\n",
       "           [ 0.3176,  0.3412,  0.3804,  ..., -0.5686, -0.5843, -0.5843],\n",
       "           [ 0.3804,  0.4196,  0.4431,  ..., -0.5608, -0.5843, -0.5765],\n",
       "           [ 0.4118,  0.4275,  0.4431,  ..., -0.5608, -0.5843, -0.5765]],\n",
       " \n",
       "          [[-0.7098, -0.7098, -0.7098,  ..., -0.6314, -0.6235, -0.6157],\n",
       "           [-0.6784, -0.6784, -0.6863,  ..., -0.5216, -0.5294, -0.5529],\n",
       "           [-0.6863, -0.6941, -0.7176,  ..., -0.4588, -0.4510, -0.4275],\n",
       "           ...,\n",
       "           [ 0.0745,  0.0980,  0.1373,  ..., -0.8118, -0.8275, -0.8196],\n",
       "           [ 0.1216,  0.1608,  0.1843,  ..., -0.8118, -0.8196, -0.8118],\n",
       "           [ 0.1529,  0.1686,  0.1843,  ..., -0.8118, -0.8196, -0.8118]]]]),\n",
       " 'text': ['A restaurant has modern wooden tables and chairs.',\n",
       "  'A man preparing desserts in a kitchen covered in frosting.']}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some cows walking across the grass and beside some trees '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[300][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82783"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': Value(dtype='string', id=None),\n",
       " 'sentids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'filename': Value(dtype='string', id=None),\n",
       " 'imgid': Value(dtype='int64', id=None),\n",
       " 'split': Value(dtype='string', id=None),\n",
       " 'sentences': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'cocoid': Value(dtype='int64', id=None),\n",
       " 'url': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_dataset = dataset['train'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(sample):\n",
    "    for i in range(len(sample[\"sentences\"])):\n",
    "        sample[f\"sentence {i+1}\"] = sample[\"sentences\"][i]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = dataset['train'].map(split_sentences, remove_columns=[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/82783 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 82783/82783 [00:01<00:00, 72267.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "new_dataset.save_to_disk('coco_karpathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 83/83 [00:02<00:00, 32.39ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48763993"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.to_json('./coco_karpathy.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "my_dataset = load_from_disk('coco_karpathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filepath', 'sentids', 'filename', 'imgid', 'split', 'cocoid', 'url', 'sentence 1', 'sentence 2', 'sentence 3', 'sentence 4', 'sentence 5'],\n",
       "    num_rows: 82783\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_images(sample):\n",
    "    sample['image'] = Image.open(os.path.join('/home/data/COCOcaptions/train2014', sample['filename']))\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  16%|█▌        | 12959/82783 [01:44<09:21, 124.30 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmy_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43madd_images\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py:3458\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3456\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3458\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m, in \u001b[0;36madd_images\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_images\u001b[39m(sample):\n\u001b[0;32m----> 2\u001b[0m     sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/data/COCOcaptions/train2014\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/PIL/Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_dataset.map(add_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': ['train2014', 'train2014', 'train2014', 'train2014', 'train2014'],\n",
       " 'sentids': [[787980, 789366, 789888, 791316, 794853],\n",
       "  [118034, 157682, 159179, 175826, 185540],\n",
       "  [813979, 814735, 816446, 816950, 817379],\n",
       "  [636271, 638152, 638287, 638446, 643555],\n",
       "  [72465, 74946, 76104, 81696, 84192]],\n",
       " 'filename': ['COCO_train2014_000000057870.jpg',\n",
       "  'COCO_train2014_000000384029.jpg',\n",
       "  'COCO_train2014_000000222016.jpg',\n",
       "  'COCO_train2014_000000520950.jpg',\n",
       "  'COCO_train2014_000000069675.jpg'],\n",
       " 'imgid': [40504, 40505, 40506, 40507, 40508],\n",
       " 'split': ['train', 'train', 'train', 'train', 'train'],\n",
       " 'cocoid': [57870, 384029, 222016, 520950, 69675],\n",
       " 'url': ['http://images.cocodataset.org/train2014/COCO_train2014_000000057870.jpg',\n",
       "  'http://images.cocodataset.org/train2014/COCO_train2014_000000384029.jpg',\n",
       "  'http://images.cocodataset.org/train2014/COCO_train2014_000000222016.jpg',\n",
       "  'http://images.cocodataset.org/train2014/COCO_train2014_000000520950.jpg',\n",
       "  'http://images.cocodataset.org/train2014/COCO_train2014_000000069675.jpg'],\n",
       " 'sentence 1': ['A restaurant has modern wooden tables and chairs.',\n",
       "  'A man preparing desserts in a kitchen covered in frosting.',\n",
       "  'a big red telephone booth that a man is standing in',\n",
       "  'the kitchen is full of spices on the rack',\n",
       "  'A child and woman are cooking in the kitchen.'],\n",
       " 'sentence 2': ['A long restaurant table with rattan rounded back chairs.',\n",
       "  'A chef is preparing and decorating many small pastries.',\n",
       "  'a person standing inside of a phone booth ',\n",
       "  'A kitchen with counter, oven and other accessories.',\n",
       "  \"A woman glances at a young girl's cooking on the stovetop\"],\n",
       " 'sentence 3': ['a long table with a plant on top of it surrounded with wooden chairs ',\n",
       "  'A baker prepares various types of baked goods.',\n",
       "  'this is an image of a man in a phone booth.',\n",
       "  'A small kitchen that utilizes all of its space. ',\n",
       "  'A young girl and a woman preparing food in a kitchen.'],\n",
       " 'sentence 4': ['A long table with a flower arrangement in the middle for meetings',\n",
       "  'a close up of a person grabbing a pastry in a container',\n",
       "  'A man is standing in a red phone booth.',\n",
       "  'This small kitchen has pots, pans and spices on display',\n",
       "  'a young person and an older person in a kitchen'],\n",
       " 'sentence 5': ['A table is adorned with wooden chairs with blue accents.',\n",
       "  'Close up of a hand touching various pastries.',\n",
       "  'A man using a phone in a phone booth.',\n",
       "  'A VERY SMALL KITCHEN WITH A STOVE AND A SHELF OF POTS ',\n",
       "  'Two women cooking on stove in a kitchen together.']}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filepath': 'train2014', 'sentids': [787980, 789366, 789888, 791316, 794853], 'filename': 'COCO_train2014_000000057870.jpg', 'imgid': 40504, 'split': 'train', 'cocoid': 57870, 'url': 'http://images.cocodataset.org/train2014/COCO_train2014_000000057870.jpg', 'sentence 1': 'A restaurant has modern wooden tables and chairs.', 'sentence 2': 'A long restaurant table with rattan rounded back chairs.', 'sentence 3': 'a long table with a plant on top of it surrounded with wooden chairs ', 'sentence 4': 'A long table with a flower arrangement in the middle for meetings', 'sentence 5': 'A table is adorned with wooden chairs with blue accents.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('coco_karpathy.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        print(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6100, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.training_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    json_objects = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            json_objects.append(json_object)\n",
    "    return json_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    seed_everything(config.lightning.seed, workers=True)\n",
    "\n",
    "    dataset = load_dataset(\"yerevann/coco-karpathy\")\n",
    "    \n",
    "    model, processor = get_models(config)\n",
    "    train_dataset, val_dataset = get_datasets(config, dataset, processor)\n",
    "    train_dataloader, val_dataloader = get_dataloaders(config, train_dataset, val_dataset)\n",
    "\n",
    "    net = LitMML(\n",
    "        model,\n",
    "        processor,\n",
    "        loss_cfg=config.loss,\n",
    "        optimizer_cfg=config.optimizer,\n",
    "        scheduler_cfg=config.scheduler,\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(**config.wandb)\n",
    "\n",
    "    # log the config on the master node\n",
    "    if rank_zero_only.rank == 0:\n",
    "        cfg_dict = OmegaConf.to_container(config, resolve=True)\n",
    "        wandb_logger.experiment.config.update(cfg_dict)\n",
    "\n",
    "    ckpt_callback = ModelCheckpoint(\n",
    "        every_n_epochs=2,\n",
    "        dirpath=f\"{config.save_dir}/ckpts/{wandb_logger.experiment.id}\",\n",
    "        filename=\"ckpt-{epoch:02d}-{val_loss:.3f}\",\n",
    "    )\n",
    "    lr_callback = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        **config.lightning.trainer,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[ckpt_callback, lr_callback]\n",
    "    )\n",
    "\n",
    "    wandb_logger.watch(net)\n",
    "\n",
    "    trainer.fit(net, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "    #trainer.fit(net, train_dataloaders=train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
