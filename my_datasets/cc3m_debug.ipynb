{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import io \n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class ConceptualCaptionsDataset:\n",
    "    def __init__(self, root, processor=None, transform=None, use_llava_split=False):\n",
    "        self.root = Path(root)\n",
    "        if not self.root.is_dir():\n",
    "            raise ValueError(\"Root must be a dir.\")\n",
    "        \n",
    "        if use_llava_split:\n",
    "            self.mapper = pd.read_parquet('/home/data/CC3M_LLaVA/mapper_LLaVA_clean.parquet')\n",
    "        else:\n",
    "            self.mapper = pd.read_parquet(self.root / 'mapper.parquet')\n",
    "        self.linker = h5py.File(self.root / 'linker.h5', 'r')\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.processor = processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mapper)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dp = self.mapper.iloc[idx]\n",
    "        \n",
    "        raw_image = self.linker.get(f\"CC3m_{dp.shard}\").get('images')[dp.h5_index]\n",
    "        image = Image.open(io.BytesIO(raw_image))\n",
    "        caption = dp.caption\n",
    "        \n",
    "        if self.processor is not None:\n",
    "            inputs = self.processor(images=image, text=caption, padding='max_length', return_tensors=\"pt\")\n",
    "            pixel_values = torch.squeeze(inputs['pixel_values'])\n",
    "            if self.transform is not None:\n",
    "                pixel_values = self.transform(pixel_values)\n",
    "            input_ids = torch.squeeze(inputs['input_ids'])\n",
    "            token_type_ids = torch.squeeze(inputs['token_type_ids'])\n",
    "            attention_mask = torch.squeeze(inputs['attention_mask'])\n",
    "\n",
    "            return pixel_values, input_ids, token_type_ids, attention_mask\n",
    "        else:\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "            return image, caption\n",
    "\n",
    "\n",
    "# from transformers import VisionTextDualEncoderProcessor, AutoImageProcessor, AutoTokenizer\n",
    "\n",
    "# processor = VisionTextDualEncoderProcessor(\n",
    "#         image_processor=AutoImageProcessor.from_pretrained('google/vit-base-patch16-224'), \n",
    "#         tokenizer=AutoTokenizer.from_pretrained('google-bert/bert-base-uncased', use_fast=False, max_length=70)\n",
    "# )\n",
    "\n",
    "# dataset = ConceptualCaptionsDataset('/home/data/mmssl/CC3m', use_llava_split=True, processor=processor)\n",
    "# mapper = dataset.mapper\n",
    "#llava_split = dataset.llava_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeite Index 0\n",
      "Verarbeite Index 10000\n",
      "Verarbeite Index 20000\n",
      "Verarbeite Index 30000\n",
      "Verarbeite Index 40000\n",
      "Warnung: Bild bei Index 42195 ist potenziell zu groß und könnte das System überlasten.\n",
      "Verarbeite Index 50000\n",
      "Verarbeite Index 60000\n",
      "Verarbeite Index 70000\n",
      "Verarbeite Index 80000\n",
      "Verarbeite Index 90000\n",
      "Warnung: Bild bei Index 99616 ist potenziell zu groß und könnte das System überlasten.\n",
      "Verarbeite Index 100000\n",
      "Verarbeite Index 110000\n",
      "Verarbeite Index 120000\n",
      "Verarbeite Index 130000\n",
      "Verarbeite Index 140000\n",
      "Verarbeite Index 150000\n",
      "Verarbeite Index 160000\n",
      "Verarbeite Index 170000\n",
      "Verarbeite Index 180000\n",
      "Verarbeite Index 190000\n",
      "Verarbeite Index 200000\n",
      "Verarbeite Index 210000\n",
      "Verarbeite Index 220000\n",
      "Verarbeite Index 230000\n",
      "Verarbeite Index 240000\n",
      "Verarbeite Index 250000\n",
      "Verarbeite Index 260000\n",
      "Verarbeite Index 270000\n",
      "Verarbeite Index 280000\n",
      "Verarbeite Index 290000\n",
      "Verarbeite Index 300000\n",
      "Verarbeite Index 310000\n",
      "Verarbeite Index 320000\n",
      "Verarbeite Index 330000\n",
      "Verarbeite Index 340000\n",
      "Verarbeite Index 350000\n",
      "Verarbeite Index 360000\n",
      "Verarbeite Index 370000\n",
      "Verarbeite Index 380000\n",
      "Verarbeite Index 390000\n",
      "Verarbeite Index 400000\n",
      "Verarbeite Index 410000\n",
      "Verarbeite Index 420000\n",
      "Verarbeite Index 430000\n",
      "Verarbeite Index 440000\n",
      "Verarbeite Index 450000\n",
      "Verarbeite Index 460000\n",
      "Verarbeite Index 470000\n"
     ]
    }
   ],
   "source": [
    "#from torchvision.transforms.functional import pil_to_tensor\n",
    "import warnings\n",
    "\n",
    "dataset = ConceptualCaptionsDataset('/home/data/mmssl/CC3m', use_llava_split=True)\n",
    "\n",
    "with open('indices_large_2.txt', 'w') as f1:\n",
    "    for idx in range(len(dataset)):\n",
    "        if idx % 10000 == 0: \n",
    "            print(f\"Verarbeite Index {idx}\")\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('error', Image.DecompressionBombWarning)\n",
    "                image, _ = dataset[idx]\n",
    "                # Weitere Bildverarbeitung hier, falls erforderlich\n",
    "        except Image.DecompressionBombWarning:\n",
    "            print(f\"Warnung: Bild bei Index {idx} ist potenziell zu groß und könnte das System überlasten.\")\n",
    "            f1.write(f\"{idx}\\n\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Ein anderer Fehler bei Index {idx}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472988\n",
      "2\n",
      "472986\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('/home/data/CC3M_LLaVA/mapper_LLaVA_clean.parquet')\n",
    "print(len(df))\n",
    "with open('indices_large_2.txt', 'r') as f:\n",
    "    indices_to_drop = [int(line.strip()) for line in f]\n",
    "    print(len(indices_to_drop))\n",
    "new_df = df.drop(df.index[indices_to_drop])\n",
    "print(len(new_df))\n",
    "new_df.to_parquet('/home/data/CC3M_LLaVA/mapper_LLaVA_clean_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ConceptualCaptionsDataset('/home/data/mmssl/CC3m', use_llava_split=True, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=192, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/PIL/Image.py:3186: DecompressionBombWarning: Image size (107736028 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/phisch/venv_py3.8/py3.8/lib/python3.8/site-packages/PIL/Image.py:3186: DecompressionBombWarning: Image size (136901120 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m      6\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/venv_py3.8/py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "for batch in dataloader:\n",
    "    pass\n",
    "end_time = time.time()\n",
    "print(f\"Time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "missing_images = []\n",
    "multiple_images = []\n",
    "ok_images = 0\n",
    "search_times = []\n",
    "idx_count = 0\n",
    "\n",
    "# start_time = time.time()\n",
    "for idx, image_name in enumerate(llava_split[\"id\"]):\n",
    "    image_id = image_name.split('_')[2]\n",
    "    start_time = time.time()\n",
    "    image_map = mapper[mapper[\"key\"] == image_id]\n",
    "    end_time = time.time()\n",
    "    search_times.append(end_time - start_time)\n",
    "    #break\n",
    "    if image_map.shape[0] == 0:\n",
    "        missing_images.append(image_id)\n",
    "    elif image_map.shape[0] > 1:\n",
    "        multiple_images.append(image_id)\n",
    "    else: ok_images += 1\n",
    "    idx_count += 1\n",
    "    # if idx == 300: break\n",
    "\n",
    "# end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 204.64686965942383\n",
      "average time for lookup: 0.6821562321980794\n"
     ]
    }
   ],
   "source": [
    "print(f\"elapsed time: {end_time - start_time}\")\n",
    "print(f\"average time for lookup: {(end_time - start_time)/300}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of lookups: {len(search_times)}\")\n",
    "print(f\"average time: {sum(search_times) / len(search_times)}\")\n",
    "search_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mapper = pd.read_parquet('/home/data/CC3M_LLaVA/mapper_LLaVA.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595375\n",
      "477329\n"
     ]
    }
   ],
   "source": [
    "print(len(llava_split))\n",
    "print(len(new_mapper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.060390472412109375\n",
      "average iter duration: 0.00020130157470703126\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "missing_images = []\n",
    "multiple_images = []\n",
    "ok_images = 0\n",
    "search_times = []\n",
    "\n",
    "start_time = time.time()\n",
    "for idx in mapper.index:\n",
    "    dp = mapper.iloc[idx]\n",
    "    if idx == 300: break\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"elapsed time: {end_time - start_time}\")\n",
    "print(f\"average iter duration: {(end_time - start_time) / 300}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
